---
title: "When the Ramp Becomes the Target: AI, Disability, and the Cost of Moral Performance"
date: "2025-09-24"
category: "AI Ethics"
tags:
  [
    "AI",
    "Disability Justice",
    "Assistive Technology",
    "Digital Equity",
    "Accessibility",
    "Privilege",
    "Health Equity",
  ]
excerpt: "For Fiona Given, a lawyer with cerebral palsy, ChatGPT isn't cheating—it's the accommodation that lets her compete. The AI panic isn't just about technology; it's about who gets to decide what counts as 'real' work."
articleType: "Contemporary Viewpoint"
---

# When the Ramp Becomes the Target: AI, Disability, and the Cost of Moral Performance - Part 2

Fiona Given is a qualified lawyer living with cerebral palsy. She has no speech and relies on augmentative and alternative communication devices to communicate. In a piece for _The Conversation_, she describes the cognitive arithmetic of daily communication:

"Each word and message that I compose takes me substantially more time and effort than a person who speaks. So I economise on that, and my written messages using current assistive technologies are often short and to the point. This can cause many problems, as I may be perceived as curt, if not rude, and I'm also not fully explaining what I mean."

ChatGPT changed that equation. Fiona tested the system and found it could add the "polite parts of emails and letters," saving her time and effort while maintaining professionalism. "Having tested the system," she writes, "ChatGPT could be particularly useful in adding the polite parts of emails and letters. It can save me time and effort whilst maintaining my professionalism."

When Fiona uses ChatGPT, she's not "cheating" at writing—she's using a tool that compensates for the physical and cognitive effort required to construct each word. But in the current climate of AI moral panic, explanations like Fiona's get drowned out by accusations that AI users are taking shortcuts or undermining intellectual authenticity.

This is what happens when we confuse assistive technology with intellectual shortcuts. And in the current climate of AI panic, this confusion isn't just awkward—it's actively harmful.

### The Ramp Metaphor We Keep Forgetting

Here's a framework that should be obvious but apparently needs repeating: for many people with disabilities, AI isn't a shortcut. It's a ramp.

Nobody accuses wheelchair users of "cheating" at walking. Nobody suggests that people using screen readers are "taking the easy way out" of reading. Nobody claims that hearing aids represent a moral failing. We understand—at least, most of us do—that these tools provide access to activities that would otherwise be difficult or impossible.

Yet when it comes to AI tools that reduce cognitive load, support communication, or assist with information processing, we suddenly become moralists. The same people who would never shame someone for using text-to-speech software are perfectly comfortable shaming them for using AI to help draft an email.

The University of Washington conducted a three-month study where researchers with disabilities tested AI tools for accessibility. One participant who is autistic found ChatGPT helpful for composing work messages without agonizing over wording for hours. Yes, colleagues found the messages "robotic"—but the tool made the researcher feel confident enough to participate in workplace communication.

Another researcher with intermittent brain fog used ChatPDF to summarize documents for work. While the tool was sometimes inaccurate (and occasionally ableist in how it framed disability), it fundamentally changed the researcher's relationship to information access. As they put it: "Using AI for this task still required work, but it lessened the cognitive load. By switching from a 'generation' task to a 'verification' task, I was able to avoid some of the accessibility issues I was facing."

Generation versus verification. That distinction matters immensely for people with certain cognitive disabilities. The energy required to generate text from scratch differs dramatically from the energy needed to evaluate and refine existing text. For many people, AI transforms an impossible task into a possible one.

<LevelingEffectViz />

### The Invisible Infrastructure

The erasure of disability from AI discourse isn't new—it's part of a much longer pattern. Remember the audiobook story from Part 1? That pattern repeats itself constantly.

![AI as Assistive Technology: Documented Accommodations in Practice](/images/ai-as-assistive-technology.jpeg)

When Be My Eyes integrated GPT-4's vision capabilities, it transformed how blind and low-vision users navigate the world. They can now point their phone camera at a restaurant menu, a medication label, a street sign—anything—and get instant, detailed descriptions. They can ask follow-up questions. They can get context that wasn't previously available.

Users report that this technology increases independence and reduces the anxiety of relying on strangers for assistance. But in the broader discourse about AI's environmental impact and ethical concerns, these users rarely get mentioned.

The National Library Service for the Blind and Print Disabled estimates that 900 million people worldwide need assistive products, including assistive technology. According to a 2023 survey by Fable, only 7% of assistive technology users believe there is adequate representation of people with disabilities in AI development—yet 87% would be willing to provide feedback to improve these tools.

Let that sink in: the community that stands to benefit most from AI accessibility features is almost completely excluded from development conversations. And now they're being shamed for using the tools that do exist.

### The Selective Outrage Problem

This brings us to the hypocrisy at the heart of AI moral panic. Why are we so focused on individual AI use when we collectively ignore environmental and social harms that dwarf it?

<HypocrisyMatrixViz />

Let's talk numbers. A typical ChatGPT query uses approximately 0.34 watt-hours of energy—roughly equivalent to a Google search (0.3 Wh). Even at higher estimates, asking ChatGPT five questions per day for an entire year would consume less energy than a single hour of video streaming.

The environmental data reveals something striking:

- One hour of Netflix streaming: 36-100 Wh
- One hour of YouTube: Similar range
- One hour of video conferencing: 2-12 liters of water
- ChatGPT query: ~30ml of water, 0.34 Wh

Yet nobody is shaming people for their Netflix binges or their Zoom happy hours. The moral intensity aimed at AI use bears no relationship to its actual environmental impact.

And that's just the environmental angle. What about fast fashion? The average person generates several kilograms of textile waste annually, consuming thousands of liters of water. What about air travel? What about the Western lifestyle built entirely on extraction from the Global South?

If we were genuinely concerned about environmental and social harm, AI would be pretty far down the list of priorities. But it's not really about the harm, is it? It's about finding a convenient target for diffuse anxieties about change, displacement, and the future.

### Health Equity in the Age of AI Panic

Now let's bring this back to digital health equity—the core concern of Mind the Gap.

In healthcare settings, AI assistive technologies are already transforming access for people with disabilities:

**Communication Support**: People with speech impairments are using AI-powered tools to construct messages more efficiently, reducing the time and cognitive energy required for basic communication. Research on ChatGPT's use for people with communication disabilities from conditions like cerebral palsy, stroke, traumatic brain injury, and motor neurone disease shows the technology can help produce clear text from poorly structured input, rewrite and improve imperfect writing, and simplify complex medical information.

**Information Processing**: Patients with cognitive impairments are using AI to help interpret medical information, understand treatment options, and communicate with healthcare providers. For people with traumatic brain injury who experience memory loss, attention deficits, and difficulty with executive functioning, AI can serve as an information retrieval tool, memory aid, and cognitive training support.

**Navigation and Orientation**: People with visual impairments are using AI vision tools to read medication labels, identify medical devices, and navigate healthcare facilities.

**Memory Assistance**: People with memory impairments from conditions like TBI or early dementia are using AI as external memory support for medication schedules, appointment reminders, and health information tracking.

These aren't frivolous uses. They're not people being "lazy" or "taking shortcuts." They're essential access needs in a healthcare system that's already incredibly difficult to navigate for anyone, let alone people with disabilities.

When we shame AI use broadly, we're not just targeting people using it for trivial tasks. We're creating an environment where people hesitate to use essential assistive technology because they fear judgment. We're making people with disabilities justify their access needs to strangers who have decided AI is morally suspicious.

### The Power Problem We're Not Discussing

Here's what gets lost in the noise of individual shaming: AI's real equity implications have nothing to do with whether individual users ask ChatGPT to summarize an article or help draft an email.

The equity questions that matter are structural:

**Who controls AI development?** Currently: predominantly Northern institutions, major tech companies, well-resourced universities. The communities most likely to be affected by AI's health equity implications—people with disabilities, populations in the Global South, marginalized communities—have minimal input.

**Whose data trains the models?** AI trained primarily on Western medical data, English-language text, and well-documented populations will systematically underperform for everyone else. This isn't hypothetical—it's already happening.

**Who has access to advanced AI health tools?** The same resource disparities that create health inequities will determine who benefits from AI-enabled healthcare improvements. Rural clinics, safety-net hospitals, and low-income communities will be the last to access beneficial AI tools—if they ever do.

**Who defines "appropriate use"?** When predominantly able-bodied people decide that AI use represents laziness or intellectual dishonesty, they're imposing their neurotypical, able-bodied assumptions on everyone else.

These are the conversations we should be having. Instead, we're wasting energy on performative morality about individual use cases.

### What We Risk Losing

The tragedy of AI moral panic isn't just that it's misdirected—it's that it could actively prevent the development of genuinely beneficial assistive applications.

If the dominant narrative around AI is "it's bad, don't use it," then:

- Researchers may avoid investigating AI assistive applications for fear of association
- Funding bodies may deprioritize AI accessibility research
- People with disabilities may hide their use of AI tools to avoid judgment
- Developers may not prioritize accessibility features in AI systems
- Healthcare systems may delay implementing AI tools that could improve access

And most critically: people who need these tools right now, today, will feel pressured to struggle without them rather than face the moral judgment of their peers.

### A Different Framework

So what's the alternative to moral panic and individual shaming?

First, we acknowledge that AI—like every technology before it—is neither inherently good nor inherently evil. It's a tool embedded in systems of power, and those systems determine how it gets used and who benefits.

Second, we center the voices of people most affected by AI's development and deployment: people with disabilities, communities in the Global South, patients navigating healthcare systems, workers whose jobs are being transformed.

Third, we redirect our energy from shaming individual users to demanding:

- Inclusive AI development with meaningful disability representation
- Transparent disclosure of AI training data sources and biases
- Equitable access frameworks that prevent AI from amplifying existing disparities
- Regulatory structures that protect vulnerable populations while enabling beneficial uses
- Research funding for AI accessibility applications in healthcare

Fourth, we recognize AI use as an accessibility issue and treat it with the same respect we (hopefully) give other assistive technologies.

### The Stakes for Digital Health Equity

In digital health, we're at a critical juncture. AI could be a powerful tool for reducing disparities—enabling remote diagnosis in underserved areas, providing decision support in resource-constrained settings, translating medical information across languages, supporting people with disabilities in managing their health.

Or it could become another sophisticated mechanism for amplifying existing inequities—available only to well-resourced institutions, trained only on privileged populations' data, designed without input from the communities most affected by health disparities.

The outcome won't be determined by how guilty individual users feel about asking ChatGPT to help write an email. It will be determined by who controls AI development, whose needs shape its design, and whether we build equity considerations into these systems from the ground up.

### Both Vigilant and Humane

We can hold multiple truths simultaneously:

- AI has genuine environmental and social impacts that require attention
- Individual AI use is a tiny fraction of those impacts
- For many people with disabilities, AI is essential assistive technology
- Shaming individual users is both ineffective and harmful
- The real equity concerns are structural, not individual
- We desperately need inclusive, thoughtful governance of AI development

What we cannot do—what we must not do—is let performative moral outrage about individual AI use distract us from the structural equity questions that actually matter.

The next time someone tells you that using AI makes you complicit in environmental destruction or intellectual dishonesty, ask them: Have they stopped streaming video? Do they fly? Do they buy fast fashion? Do they live a Western lifestyle built on global extraction?

And if they answer "yes" to those questions—if they've truly divested from all the normalized harms we collectively accept—then sure, their moral high ground has some legitimacy. But for the rest of us, the hypocrisy is showing.

More importantly, ask them: Would they say the same thing to someone using a screen reader? A hearing aid? A wheelchair? Because for many people, that's exactly what AI represents—a tool that makes participation possible.

The choice isn't between embracing AI uncritically or rejecting it wholesale. It's between directing our attention toward the structural questions that determine whether AI amplifies justice or inequality, and wasting it on moral theater that mostly serves to make people with disabilities feel bad about using essential tools.

We know which choice advances equity. The question is whether we're brave enough to make it.

---

**Sources**: This analysis draws on research from Fiona Given and colleagues (_The Conversation_), University of Washington disability studies, Fable Technology Labs, MIT environmental research, Hannah Ritchie's environmental data analysis, the International Energy Agency, and disability advocacy organizations including The A11Y Project.
